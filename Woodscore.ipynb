{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc2b5073",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ksmith/anaconda3/envs/evaluatedev/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import accuracy_score\n",
    "from scipy import spatial\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity \n",
    "from sklearn.datasets import fetch_20newsgroups\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c60642a",
   "metadata": {},
   "source": [
    "## Computing Matrix Multiplication of two embedding matricies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8e3c3a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define two large embedding matricies\n",
    "\n",
    "matrix_1 = torch.rand(50, 768)\n",
    "matrix_2 = torch.rand(45, 768)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cebb8c2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50, 45)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matrix_1 = matrix_1.cpu().detach().numpy()     \n",
    "matrix_2 = matrix_2.cpu().detach().numpy()     \n",
    "cosine_sim = cosine_similarity(matrix_1, matrix_2)\n",
    "cosine_sim.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dd94e3a",
   "metadata": {},
   "source": [
    "## Running on Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1965ff31",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer('all-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2089790c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cats = ['alt.atheism', 'sci.space']\n",
    "newsgroups_train = fetch_20newsgroups(subset='train', categories=cats)\n",
    "\n",
    "newsgroups_test = fetch_20newsgroups(subset='test',\n",
    "                                 categories=cats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a351e150",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1073"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(newsgroups_train.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dc3b3977",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compute embedding for both lists\n",
    "train= model.encode(newsgroups_train.data, convert_to_tensor=True)\n",
    "test = model.encode(newsgroups_test.data, convert_to_tensor=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "55aaa43d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1073, 713)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matrix_1 = train.cpu().detach().numpy()     \n",
    "matrix_2 = test.cpu().detach().numpy()     \n",
    "cosine_sim = cosine_similarity(matrix_1, matrix_2)\n",
    "cosine_sim.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ac2d84c",
   "metadata": {},
   "source": [
    "## Finding the Average Cosine Similarity Across First Axis?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e881b09d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50, 713)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we'd want to find the sum of the top-n for each row\n",
    "cosine_sim[:50].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "38826222",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(713,)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# how do we find the sum of only the first \"b\" of each row of array\n",
    "avg_matrix = np.sum(cosine_sim[:50], axis = 0)\n",
    "# sort in place by descending - https://stackoverflow.com/questions/26984414/efficiently-sorting-a-numpy-array-in-descending-order \n",
    "avg_matrix[::-1].sort()\n",
    "\n",
    "avg_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "07ddede4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11.210295"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_matrix[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ead2c6de",
   "metadata": {},
   "source": [
    "## Finding Wood Score across dummy inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3eb30ee8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nWhat we're worried about now is that we need to include p in the sample weights for accuracy... that either means we factor it\\ninto the avg matrix which would be preferrable or to calculate accuracy by hand which I dont like as much...\\n\\n\""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "What we're worried about now is that we need to include p in the sample weights for accuracy... that either means we factor it\n",
    "into the avg matrix which would be preferrable or to calculate accuracy by hand which I dont like as much...\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5a0783e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(713,)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newsgroups_test.target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8e2c9655",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = np.zeros((713,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "59c925dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4474053295932679"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(newsgroups_test.target, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "13e18b7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wood Score v1 = 1.152342695842117!\n"
     ]
    }
   ],
   "source": [
    "p_matrix = 15 / avg_matrix\n",
    "accuracies = []\n",
    "for count, value in enumerate(newsgroups_test.target):\n",
    "    accuracy = accuracy_score([value], [int(pred[count])])\n",
    "    p = p_matrix[count]\n",
    "    accuracies.append(accuracy*p)\n",
    "    \n",
    "result = sum(accuracies) / len(accuracies)\n",
    "print(f'Wood Score v1 = {result}!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64a33f63",
   "metadata": {},
   "source": [
    "## What is a good value for \"A\"?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79fdc8bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The main equation that we have to worry about here is - a / sum(max_b of similarity(train/test))\n",
    "\n",
    "\n",
    "Cosine similarity takes on values between 0 and 1 meaning that the denomenator will take on a value somewhere between (0, b)\n",
    "\n",
    "For stability we may want to add a max(sum(max_b), np.eps) to avoid division by 0?\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42840a81",
   "metadata": {},
   "source": [
    "## How to use sentence transformers for STS???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5fefc02f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "sentences = [\"I'm happy\", \"I'm full of happiness\"]\n",
    "\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "#Compute embedding for both lists\n",
    "embedding_1= model.encode(sentences[0], convert_to_tensor=True)\n",
    "embedding_2 = model.encode(sentences[1], convert_to_tensor=True)\n",
    "\n",
    "test = util.pytorch_cos_sim(embedding_1, embedding_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "25bf46ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.6003, device='cuda:0')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_norm = embedding_1 / embedding_1.norm()\n",
    "b_norm = embedding_2 / embedding_2.norm()\n",
    "res = a_norm @ b_norm.T\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "570cc7ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.6003, device='cuda:0')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_1@embedding_2.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "abaf3936",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6002568602561951"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "572233e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = model.encode(sentences, convert_to_tensor = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c87b5b81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 384])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9002f0bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.9789,  0.0256, -0.1723,  0.2196,  0.3638, -0.9793,  0.7141, -0.0123,\n",
       "         -0.6612,  0.5094,  0.9746, -0.5441,  0.9950,  0.9288,  0.5303,  0.3601,\n",
       "          0.8983, -0.8832,  0.0420,  0.7086, -0.7811,  0.2444,  0.0368, -0.0227,\n",
       "          0.4534, -0.0326,  0.9171, -0.9680,  0.9405, -0.4979, -0.9880,  0.9960,\n",
       "          0.7064, -0.9897, -0.6682, -0.7107, -0.7046, -0.6136,  0.9078, -0.8851,\n",
       "         -0.0738, -0.9702,  0.1700,  0.2970, -0.5985,  0.9864,  0.6968,  0.3486,\n",
       "          0.6120, -0.8676,  0.3502,  0.3012, -0.6626, -0.9267,  0.6914,  0.8699,\n",
       "          0.5964,  0.9408, -0.9999, -0.8361,  0.9981,  0.5822, -0.9998,  0.9626,\n",
       "          0.3409,  0.5239, -0.8711,  0.1554, -0.4778,  0.5798,  0.9586,  0.3363,\n",
       "          0.5820, -0.9199,  0.9971,  0.9649,  0.8329,  0.7814,  0.5148,  0.6839,\n",
       "          0.8625, -0.7379, -0.7334, -0.8007, -0.8483, -0.9305, -0.4614,  0.6405,\n",
       "         -0.7090,  0.8925, -0.2260,  0.2287,  0.8580, -0.8629, -0.7607, -0.8898,\n",
       "         -0.2928,  0.9413, -0.6632,  0.7089, -0.9620,  0.9599,  0.9989, -0.9631,\n",
       "         -0.8163,  0.5262, -0.3826,  0.4906, -0.1731, -0.7056,  0.2564,  0.8717,\n",
       "          0.6320,  0.9315, -0.6996,  0.9908, -0.7951,  0.9693,  0.7921, -0.9412,\n",
       "         -0.6264, -0.3563,  0.8846,  0.4439, -0.7420, -0.8415, -0.3467,    -inf,\n",
       "         -0.7036,  0.7019,  0.4687,  0.6764,  0.8514,  0.2135,  0.9719, -0.8475,\n",
       "         -0.7029, -0.3348, -0.7810,  0.6990, -0.2956,  0.5915, -0.4899, -0.8875,\n",
       "         -0.9875,  0.4556, -0.3112,  0.7067, -0.7447, -0.0414,  0.1575,  0.2799,\n",
       "          0.9737,  0.4997,  0.5431, -0.8127,  0.5827,  0.6121, -0.8198,  0.6009,\n",
       "          0.8450, -0.0458, -0.9786, -0.9259, -0.7727, -0.9986, -0.9434,  0.9359,\n",
       "         -0.5098,  0.7327, -0.8466, -0.7155, -0.8090,  0.8597, -0.9884,  0.7877,\n",
       "         -0.6262, -0.9877, -0.1279, -0.0639,  0.9863,  0.1304, -0.7242, -0.2779,\n",
       "         -0.9227,  0.5907,  0.7132,  0.9970,  0.2186,  0.9812, -0.3931, -0.8764,\n",
       "         -0.9970,  0.7731,  0.1616,  0.0901,  0.6706, -0.8512, -0.8621,  0.6076,\n",
       "          0.7115,  0.2720,  0.5861,  0.6114, -0.9312, -0.9952, -0.0740, -0.9197,\n",
       "          0.9180, -0.4735,  0.9642, -0.7213,  0.5563,  0.8778, -0.4729, -0.5153,\n",
       "         -0.5947,  0.5804,  0.8906,  0.9902,  0.6638,  0.6048, -0.5776,     inf,\n",
       "          0.5646,  0.8008, -0.7227,  0.9247, -0.8898, -0.9815,  0.1471,  0.8302,\n",
       "         -0.3362,  0.1585,  0.6519, -0.2116,  0.0530,  0.7325, -0.7261,  0.9704,\n",
       "          0.6942, -0.9152, -0.4927, -0.6746, -0.4358,  0.9465, -0.8262,  0.8084,\n",
       "         -0.5720,  0.9518,  0.7843, -0.9019,  0.8468,  0.8394,  0.2758,  0.3962,\n",
       "         -0.4750, -0.9075,  0.5034, -0.1039,  0.3504,  0.6598, -0.8948,  0.9479,\n",
       "          0.9768, -0.7716,  0.3007,  0.8183,  0.1628,  0.5033,  0.4651,  0.9989,\n",
       "          0.7030,  0.9665, -0.9288, -0.9935, -0.9605, -0.5672, -0.2542, -0.4626,\n",
       "         -0.7652, -0.8762, -0.9749, -0.1598, -0.6993, -0.2499, -0.6948, -0.8640,\n",
       "          0.7399, -0.5478,  0.7626, -0.8165, -0.5063,  0.8648, -0.5726,  0.7604,\n",
       "         -0.8546, -0.7168,  0.4052, -0.6361,  0.4181,  0.9736,  0.7689, -0.9487,\n",
       "         -0.7221,  0.8154,  0.6772,  0.7074, -0.1998,  0.1608, -0.2140, -0.4268,\n",
       "          0.9256,  0.9516, -0.9121,  0.1740, -0.9870, -0.7984,  0.7856, -0.7176,\n",
       "          0.9027,  0.3395, -0.7469, -0.4964,  0.9944,  0.6556,  0.2913, -0.5840,\n",
       "         -0.4111,  0.3720,  0.9415,  0.7560, -0.8332, -0.8864,  0.6058,  0.3587,\n",
       "          0.5384, -0.5211,  0.1309,  0.3120, -0.6567, -0.2107, -0.9638,  0.9681,\n",
       "         -0.7550,  0.3454, -0.9962,  0.6617, -0.8092,  0.4298,  0.1289,  0.9043,\n",
       "         -0.6905,  1.0000,  0.0273,  0.0659,  0.6083,  0.7920,  0.9993,  0.3699,\n",
       "          0.0984, -0.1767, -0.7007,  0.8393, -0.9656, -0.2833,  0.7724,  0.7328,\n",
       "         -0.3134, -0.4178,  0.9595,  0.8091, -0.8859,  0.8104,  0.9955,  0.7507,\n",
       "          0.2857,  0.2172,  0.5480,  0.2361,  0.3820,  0.9999,  1.0000, -0.5445],\n",
       "        [ 0.2045,  0.9997,  0.9850,  0.9756,  0.9315, -0.2025,  0.7001,  0.9999,\n",
       "          0.7502, -0.8605, -0.2238, -0.8390, -0.0995, -0.3705,  0.8478,  0.9329,\n",
       "          0.4394, -0.4690, -0.9991, -0.7056, -0.6244,  0.9697, -0.9993,  0.9997,\n",
       "         -0.8913,  0.9995,  0.3986, -0.2511,  0.3399, -0.8672,  0.1545,  0.0898,\n",
       "          0.7078, -0.1433, -0.7440, -0.7034, -0.7096, -0.7896,  0.4194, -0.4655,\n",
       "         -0.9973, -0.2425,  0.9854,  0.9549, -0.8011,  0.1645,  0.7173, -0.9373,\n",
       "          0.7908, -0.4973,  0.9367,  0.9536, -0.7490,  0.3757,  0.7225,  0.4932,\n",
       "          0.8027,  0.3389, -0.0106,  0.5486, -0.0622,  0.8130, -0.0201,  0.2711,\n",
       "          0.9401, -0.8518, -0.4911,  0.9878, -0.8785,  0.8148, -0.2846, -0.9418,\n",
       "         -0.8132,  0.3921,  0.0765,  0.2627,  0.5534, -0.6240,  0.8573,  0.7296,\n",
       "          0.5060, -0.6750, -0.6798, -0.5991, -0.5295,  0.3663, -0.8872,  0.7679,\n",
       "         -0.7052,  0.4510, -0.9741,  0.9735,  0.5136, -0.5054, -0.6491, -0.4564,\n",
       "         -0.9562, -0.3375, -0.7484,  0.7053,  0.2731, -0.2804,  0.0468, -0.2692,\n",
       "         -0.5777,  0.8503, -0.9239,  0.8714,  0.9849, -0.7087, -0.9666, -0.4901,\n",
       "          0.7750,  0.3638,  0.7146, -0.1350, -0.6065,  0.2459,  0.6104, -0.3379,\n",
       "          0.7795,  0.9344,  0.4664,  0.8961, -0.6704, -0.5403, -0.9380,    -inf,\n",
       "         -0.7106,  0.7122,  0.8834,  0.7366, -0.5245,  0.9769,  0.2354, -0.5308,\n",
       "         -0.7113,  0.9423, -0.6245,  0.7151, -0.9553,  0.8063, -0.8718, -0.4608,\n",
       "         -0.1575,  0.8902,  0.9503,  0.7075, -0.6674, -0.9991,  0.9875,  0.9600,\n",
       "         -0.2280, -0.8662,  0.8397, -0.5827, -0.8127, -0.7908, -0.5727,  0.7993,\n",
       "          0.5348, -0.9989, -0.2058, -0.3777, -0.6347,  0.0534, -0.3316, -0.3522,\n",
       "         -0.8603,  0.6805, -0.5322,  0.6986, -0.5878,  0.5107, -0.1519,  0.6160,\n",
       "         -0.7797,  0.1566, -0.9918, -0.9980,  0.1648, -0.9915, -0.6896, -0.9606,\n",
       "          0.3854,  0.8069,  0.7010,  0.0768, -0.9758, -0.1928, -0.9195, -0.4816,\n",
       "         -0.0768,  0.6343,  0.9869, -0.9959,  0.7419,  0.5249,  0.5068,  0.7942,\n",
       "          0.7027, -0.9623, -0.8102, -0.7913, -0.3645,  0.0982, -0.9973, -0.3927,\n",
       "          0.3967,  0.8808,  0.2651, -0.6926,  0.8310, -0.4791, -0.8811, -0.8570,\n",
       "         -0.8039,  0.8143,  0.4548,  0.1396,  0.7480, -0.7964, -0.8163,     inf,\n",
       "          0.8254, -0.5989, -0.6912,  0.3807, -0.4563,  0.1913, -0.9891,  0.5575,\n",
       "         -0.9418,  0.9874,  0.7583, -0.9774,  0.9986,  0.6807, -0.6876, -0.2414,\n",
       "          0.7198,  0.4030, -0.8702, -0.7382, -0.9000,  0.3227, -0.5634,  0.5887,\n",
       "          0.8202,  0.3067,  0.6204, -0.4318,  0.5320,  0.5435,  0.9612, -0.9182,\n",
       "         -0.8800, -0.4201,  0.8640, -0.9946, -0.9366,  0.7514, -0.4465,  0.3187,\n",
       "         -0.2139, -0.6362,  0.9537,  0.5748,  0.9867, -0.8641,  0.8852,  0.0458,\n",
       "          0.7112,  0.2568,  0.3705, -0.1138,  0.2783,  0.8236,  0.9672, -0.8866,\n",
       "         -0.6438, -0.4819, -0.2229, -0.9871, -0.7148,  0.9683, -0.7192, -0.5035,\n",
       "          0.6727, -0.8366,  0.6469, -0.5774, -0.8623, -0.5021, -0.8198,  0.6494,\n",
       "         -0.5193,  0.6973, -0.9142,  0.7716,  0.9084,  0.2283, -0.6394,  0.3163,\n",
       "         -0.6918, -0.5789,  0.7358,  0.7068, -0.9798,  0.9870, -0.9768,  0.9043,\n",
       "          0.3784, -0.3075, -0.4101,  0.9848,  0.1608, -0.6021,  0.6188, -0.6965,\n",
       "         -0.4302, -0.9406, -0.6650, -0.8681,  0.1061,  0.7551,  0.9566,  0.8118,\n",
       "         -0.9116,  0.9282,  0.3369,  0.6545, -0.5530, -0.4630,  0.7956,  0.9335,\n",
       "          0.8427, -0.8535,  0.9914,  0.9501,  0.7541, -0.9776, -0.2666, -0.2504,\n",
       "         -0.6558, -0.9385, -0.0875, -0.7498, -0.5875,  0.9029,  0.9917,  0.4268,\n",
       "         -0.7234, -0.0021,  0.9996, -0.9978, -0.7937, -0.6105, -0.0369,  0.9291,\n",
       "         -0.9951,  0.9843,  0.7135,  0.5437, -0.2599,  0.9590,  0.6352,  0.6804,\n",
       "         -0.9496, -0.9085, -0.2818, -0.5877,  0.4638,  0.5858, -0.0949,  0.6607,\n",
       "          0.9583,  0.9761, -0.8365,  0.9717,  0.9242,  0.0151, -0.0040, -0.8388]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings / embeddings.norm(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b2a404a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1758, -0.2684,  2.3224],\n",
       "        [ 0.0442,  0.4949, -0.0749],\n",
       "        [ 0.8558, -0.3579, -1.0935],\n",
       "        [-1.0837, -0.0103,  1.3125]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.randn(4, 3)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "98b99b84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.2340, -0.7446, -0.1310],\n",
      "        [ 0.6339,  0.9609, -0.3136]])\n",
      "[[-0.2340468  -0.74459177 -0.13096361]\n",
      " [ 0.63388145  0.96090335 -0.31360054]]\n"
     ]
    }
   ],
   "source": [
    "a = torch.randn(2, 2)\n",
    "b = torch.randn(3, 2) # different row number, for the fun\n",
    "\n",
    "# Given that cos_sim(u, v) = dot(u, v) / (norm(u) * norm(v))\n",
    "#                          = dot(u / norm(u), v / norm(v))\n",
    "# We fist normalize the rows, before computing their dot products via transposition:\n",
    "a_norm = a / a.norm(dim=1)[:, None]\n",
    "b_norm = b / b.norm(dim=1)[:, None]\n",
    "res = torch.mm(a_norm, b_norm.transpose(0,1))\n",
    "print(res)\n",
    "#  0.9978 -0.9986 -0.9985\n",
    "# -0.8629  0.9172  0.9172\n",
    "\n",
    "# -------\n",
    "# Let's verify with numpy/scipy if our computations are correct:\n",
    "a_n = a.numpy()\n",
    "b_n = b.numpy()\n",
    "res_n = np.zeros((2, 3))\n",
    "for i in range(2):\n",
    "    for j in range(3):\n",
    "        # cos_sim(u, v) = 1 - cos_dist(u, v)\n",
    "        res_n[i, j] = 1 - spatial.distance.cosine(a_n[i], b_n[j])\n",
    "print(res_n)\n",
    "# [[ 0.9978022  -0.99855876 -0.99854881]\n",
    "#  [-0.86285472  0.91716063  0.9172349 ]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "184dea32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.24105462431907654"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mean(a, 1)[0].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "1347949e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences1 = [\"I'm happy\", \"I'm full of happiness\"]\n",
    "\n",
    "sentences2 = [\"I'm sad\", \"I'm full of\"]\n",
    "\n",
    "embedding_1= model.encode(sentences1, convert_to_tensor=True)\n",
    "embedding_2 = model.encode(sentences2, convert_to_tensor=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "46883f9f",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (384) must match the size of tensor b (2) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[64], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m a_norm \u001b[38;5;241m=\u001b[39m \u001b[43membedding_1\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43membedding_1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdim\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m b_norm \u001b[38;5;241m=\u001b[39m embedding_2 \u001b[38;5;241m/\u001b[39m embedding_2\u001b[38;5;241m.\u001b[39mnorm(dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m      3\u001b[0m res \u001b[38;5;241m=\u001b[39m a_norm \u001b[38;5;241m@\u001b[39m b_norm\u001b[38;5;241m.\u001b[39mT\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (384) must match the size of tensor b (2) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "a_norm = embedding_1 / embedding_1.norm(dim = 0)\n",
    "b_norm = embedding_2 / embedding_2.norm(dim = 0)\n",
    "res = a_norm @ b_norm.T\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a1473514",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 1.], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "output = F.cosine_similarity(embedding_1, embedding_2, dim = 1)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "369b3c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor([[1,2],[3,4]])\n",
    "b = torch.tensor([[1,2], [4,5], [7,8]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "41ed2b09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 5, 14, 23],\n",
       "        [11, 32, 53]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a@b.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1928e8fc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
